import os
import sys

#sys.path.append(os.path.abspath("..."))
import torch
import torchvision
from torch.utils.data import DataLoader, Dataset
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import random
import os.path as osp
import matplotlib.pyplot as plt
from typing import List
import argparse
from PIL import ImageFilter
from torchvision import datasets, transforms
from dataset.cifar_c_dataset import NORM, cifar100_c_testset, distrb_dict, te_transforms
from methods.pda import PDA
import models.model_load as model_load
import models.Res as Res
from utils.valid_func import validate, pse_label_distri_dist
from utils.cli_utils import AverageMeter
from dataset.sade_data_loader.imbalance_cifar import DirichletImbalanceCIFAR100, DirichletImbalanceCIFAR10

# from dataset.sade_data_loader.cifar_data_loaders import DirichletImbalanceCIFAR100DataLoader
class_num = 100


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True


def gen_output(model, train_loader, feat_out=True, out_dir=None):
    # gen feature
    train_output_list = [[], [], []]
    with torch.no_grad():
        for images, targets in train_loader:
            images = images.cuda()
            # targets = targets.cuda()
            if feat_out:
                output = model(images, feature_flag=True)
            else:
                output = model(images)
            for f, l in zip(output, train_output_list):
                l.append(f.cpu())

    train_feat = torch.stack([torch.cat(l) for l in train_output_list])
    if out_dir is not None:
        torch.save(train_feat, out_dir)
    return train_feat


class GaussianBlur(object):
    def __init__(self, sigma=[.1, 2.]):
        self.sigma = sigma

    def __call__(self, x):
        if random.random() < 0.2:
            sigma = random.uniform(self.sigma[0], self.sigma[1])
            x = x.filter(ImageFilter.GaussianBlur(radius=sigma))
        return x


class TwoCropsTransform:
    def __init__(self, base_transform):
        self.base_transform = base_transform

    def __call__(self, x):
        q = self.base_transform(x)
        k = self.base_transform(x)
        return [q, k]


class DirichletImbalanceCIFAR100DataLoader(DataLoader):

    def __init__(self, custom_dataset, batch_size, shuffle=True, num_workers=1, training=True, balanced=False,
                 retain_epoch_size=True, imb_type='exp', imb_factor=0.01, test_imb_factor=0, reverse=False,
                 ):
        self.dataset = custom_dataset
        self.val_dataset = custom_dataset

        normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],
                                         std=[0.2023, 0.1994, 0.2010])
        train_trsfm = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomApply([
                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
            ], p=0.8),
            # transforms.RandomGrayscale(p=0.1),
            transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.2),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ])
        test_trsfm = transforms.Compose([
            transforms.ToTensor(),
            normalize,
        ])

        if not training:
            self.dataset = DirichletImbalanceCIFAR100(
                custom_dataset, train=False, download=True, transform=test_trsfm,
                imb_type=imb_type, imb_factor=test_imb_factor, reverse=reverse
            )
            self.val_dataset = self.dataset

        num_classes = len(np.unique(self.dataset.targets))
        assert num_classes == 100

        cls_num_list = [0] * num_classes
        for label in self.dataset.targets:
            cls_num_list[label] += 1

        self.cls_num_list = cls_num_list

        self.shuffle = shuffle
        self.init_kwargs = {
            'batch_size': batch_size,
            'shuffle': self.shuffle,
            'num_workers': num_workers
        }

        super().__init__(dataset=self.dataset, **self.init_kwargs)

    def train_set(self):
        return DataLoader(dataset=self.dataset, shuffle=True, **self.init_kwargs)

    def test_set(self):
        return DataLoader(dataset=self.val_dataset, shuffle=False, **self.init_kwargs)

    def get_test_cls_num_list(self):
        if hasattr(self.val_dataset, "get_cls_num_list"):
            return self.val_dataset.get_cls_num_list()
        else:
            raise AttributeError("Validation dataset does not implement 'get_cls_num_list'.")


class LDE(nn.Module):
    def __init__(self, input_dim, output_dim, ensemble=1):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, output_dim * 3),
            nn.ReLU(),
            nn.Linear(output_dim * 3, output_dim),
        )
        self.ensemble = ensemble
        self.in_dim = input_dim
        self.out_dim = output_dim

    def forward(self, input):
        output = self.mlp(input)
        return output

class Bayesian_LDE(nn.Module):
    def __init__(self, input_dim, output_dim, num_samples=10):
        super().__init__()
        self.num_samples = num_samples

        self.mlp = nn.Sequential(
            nn.Linear(input_dim, output_dim * 3),
            nn.ReLU(),
            nn.Dropout(0.5),  # Monte Carlo Dropout
            nn.Linear(output_dim * 3, output_dim),
        )
    def forward(self, input):

        rough_outputs = torch.stack([self.mlp(input) for _ in range(5)], dim=0)
        rough_uncertainty = rough_outputs.std(dim=0)

        adaptive_samples = min(50, int(5 + (rough_uncertainty.mean().item() * 20)))
        outputs = torch.stack([self.mlp(input) for _ in range(adaptive_samples)], dim=0)

        mean_output = outputs.mean(dim=0)
        uncertainty = outputs.std(dim=0)
        return mean_output, uncertainty


def gen_imbalanced_data(feat, img_num_per_cls, targets, target_one_hot, idx_each_class: List[torch.Tensor],
                        classes=None):
    if classes is None:
        classes = torch.unique(targets)
    chosen_idx_list = []

    for the_class, the_img_num in zip(classes, img_num_per_cls):
        idx = idx_each_class[the_class]
        selec_idx = torch.multinomial(torch.ones(idx.size(0)), the_img_num, replacement=True)
        chosen_idx = idx[selec_idx]
        chosen_idx_list.append(chosen_idx)

    chosen_idx = torch.cat(chosen_idx_list)
    feat = feat[chosen_idx]
    feat_mean = feat.mean(dim=0)
    label_onehot = target_one_hot[chosen_idx]
    distri_target = label_onehot.mean(dim=0)
    return feat_mean, distri_target


class LDDateset(Dataset):
    def __init__(self, feat: torch.Tensor, label: torch.Tensor, imb_ub=100.0, step=.1, load_path=None, clc_num=100,
                 sampling_method='single', imb_factor=0.01, reverse=False, seed=None):
        super().__init__()
        self.feat = feat
        self.label = label
        self.imb_ub = imb_ub
        self.step = step
        self.distri_list = []
        self.distri_targets = []
        self.sampling_method = sampling_method
        self.seed = seed

        if self.seed is not None:
            random.seed(self.seed)
            torch.manual_seed(self.seed)

        cur_imb = 1.0
        self.clc_num = clc_num
        # targets_np = np.array(self.label, dtype=np.int64)
        self.label_one_hot = F.one_hot(label).float()
        self.num_cls_list = []

        time_start = time.time()
        if load_path is None or not osp.exists(osp.join(load_path, 'distri_list')):
            classes = torch.unique(label)
            idx_each_class = []
            img_max = 0
            for the_class in classes:
                idx = torch.nonzero(label == the_class).squeeze(1)
                idx_each_class.append(idx)
                idx_len = len(idx)
                self.num_cls_list.append(idx_len)
                img_max = max(img_max, idx_len)

            f1_flag = True
            f2_flag = True
            while cur_imb < imb_ub:
                img_num_per_cls1 = self.set_imbalance_data(img_max, cur_imb, False, self.clc_num)
                img_num_per_cls2 = [i for i in reversed(img_num_per_cls1)]
                img_num_per_cls1_tensor = torch.tensor(img_num_per_cls1)
                img_num_per_cls2_tensor = torch.tensor(img_num_per_cls2)

                feat1, distri1 = gen_imbalanced_data(self.feat, img_num_per_cls1_tensor, label, self.label_one_hot,
                                                     classes=classes, idx_each_class=idx_each_class)
                self.distri_list.append(feat1)
                self.distri_targets.append(distri1)
                feat2, distri2 = gen_imbalanced_data(self.feat, img_num_per_cls2_tensor, label, self.label_one_hot,
                                                     classes=classes, idx_each_class=idx_each_class)
                self.distri_list.append(feat2)
                self.distri_targets.append(distri2)

                cur_imb += step
            time_cost = time.time() - time_start
            print(f'It takes {time_cost} sec to complete dataset')
        else:
            self.load_param(load_path)
        return

    def __len__(self):
        return len(self.distri_list)

    def __getitem__(self, item):
        return self.distri_list[item], self.distri_targets[item]

    def save_param(self, file_path):
        os.makedirs(str(file_path), exist_ok=True)
        torch.save(self.distri_list, osp.join(str(file_path), 'distri_list'))
        torch.save(self.distri_targets, osp.join(str(file_path), 'distri_targets'))

    def load_param(self, file_path):
        self.distri_list = torch.load(osp.join(file_path, 'distri_list'))
        self.distri_targets = torch.load(osp.join(file_path, 'distri_targets'))


    @staticmethod
    def set_imbalance_data(img_max, imb_factor, reverse, cls_num,alpha_min=0.1, alpha_max=10, num_samples=5):
        img_num_per_cls = []
        if isinstance(imb_factor, list):
            for i, factor in enumerate(imb_factor):
                if reverse:
                    num = img_max * (factor ** ((cls_num - 1 - i) / (cls_num - 1.0)))
                else:
                    num = img_max * (factor ** (i / (cls_num - 1.0)))
                img_num_per_cls.append(int(num))
        else:
            if imb_factor > 1.0:
                reverse = not reverse
                imb_factor = 1. / imb_factor
            for cls_idx in range(cls_num):
                if reverse:
                    num = img_max * (imb_factor ** ((cls_num - 1 - cls_idx) / (cls_num - 1.0)))
                else:
                    num = img_max * (imb_factor ** (cls_idx / (cls_num - 1.0)))
                img_num_per_cls.append(int(num))


        alpha = np.array(img_num_per_cls, dtype=np.float64)
        alpha = alpha / alpha.sum() * img_max
        alpha = np.clip(alpha, alpha_min, alpha_max)

        sampled_distributions = [np.random.dirichlet(alpha) for _ in range(num_samples)]
        sampled_distribution = np.mean(sampled_distributions, axis=0)

        pro_max = max(sampled_distribution)

        img_num_per_cls = [max(int(x / pro_max * img_max), 2) for x in sampled_distribution]

        return img_num_per_cls

    def dirichlet_mixture_sampling(self, img_max, cls_num):
        distrb = {
            'uniform': (1, False),
            'forward': (0.01, False),
            'backward': (0.01, True)
        }
        test_distribution_set = ['uniform', 'forward', 'backward']
        p1, p2, p3 = 1 / 3, 1 / 3, 1 / 3

        test_distribution = np.random.choice(test_distribution_set, p=[p1, p2, p3])
        imb_factor = distrb[test_distribution][0]
        reverse = distrb[test_distribution][1]
        print(f"[DEBUG] Selected Distribution: {test_distribution}, Reverse={reverse}")

        print(f"[Sampling] Using Dirichlet Mixture Sampling: Distribution={test_distribution}")

        # Generate class sample numbers
        img_num_per_cls = self.set_imbalance_data(img_max, imb_factor, reverse, cls_num)
        img_num_per_cls = [max(num, 2) for num in img_num_per_cls]
        print(f"[Sampling] Using {test_distribution} Distribution: {img_num_per_cls}")
        return img_num_per_cls


    @staticmethod
    def get_topK(logits, label, tol=.9):
        num_sample, num_label = logits.shape
        label_expanded = label.unsqueeze(1).expand_as(logits)
        top_k_values, top_indices = torch.topk(logits, num_label, dim=1)
        k = 1
        for k in range(1, num_label + 1):
            comparison = label_expanded[:, :k] == top_indices[:, :k]
            pos_ratio = (comparison.any(dim=1)).sum() / num_sample
            if pos_ratio > tol:
                break
        return k

    @staticmethod
    def topk_mask(tensor, k=5, dim=1):
        values, indices = tensor.topk(k, dim=dim)
        mask = torch.zeros_like(tensor)
        mask.scatter_(dim, indices, 1)
        result = tensor * mask

        return result


def mask_topk(x, label, tol=0.9):
    chosen_k = LDDateset.get_topK(x, label=label, tol=tol)
    masked_x = LDDateset.topk_mask(x, k=chosen_k, dim=1)
    return masked_x, chosen_k


def proc_ide_logits_list(train_logits, label, top_k='ada', tol=0.9):
    ide_logits_list = []
    for i in range(3):
        if top_k == 'ada':
            chosen_k = LDDateset.get_topK(train_logits[i], label=label, tol=tol)
        else:
            chosen_k = top_k
        print(f"chosen_k: {chosen_k}", end='\t')
        logits_item = LDDateset.topk_mask(train_logits[i], k=chosen_k, dim=1)
        ide_logits_list.append(logits_item)
    print("")
    ide_logits = torch.hstack(ide_logits_list)
    return ide_logits


def proc_logtis_and_label_list(load_root_dir_list, top_k, tol):
    l_list, t_list = [], []
    for root_dir in load_root_dir_list:
        l = torch.load(os.path.join(root_dir, 'logits'), 'cpu')
        t = torch.load(os.path.join(root_dir, 'label'), 'cpu')
        l_list.append(l)
        t_list.append(t)

    ide_logits_list = []
    for l, t in zip(l_list, t_list):
        ide_logits_list.append(proc_ide_logits_list(l, t, top_k, tol=tol))

    label = torch.cat(t_list)
    ide_logits = torch.cat(ide_logits_list)
    return ide_logits, label


def get_dataset_wrap(feat_dir, top_k, save_data_path=None, tol=.95, force_gen_data=True, step=.1,
                     sampling_method='single'):
    load_root_dir_list = [feat_dir]
    ide_logits, label = proc_logtis_and_label_list(load_root_dir_list, top_k, tol)
    if osp.exists(osp.join(save_data_path, 'distri_list')) and not force_gen_data:
        print("[Dataset Wrap] Loading saved dataset...")
        ide_train_set = LDDateset(ide_logits, label, imb_ub=100.0, step=step, load_path=save_data_path)
    else:
        print(f"[Dataset Wrap] Generating new dataset with sampling_method={sampling_method}")
        distrb = {
            'uniform': (1, False),
            'forward': (0.01, False),
            'backward': (0.01, True)
        }
        test_distribution_set = ['uniform', 'forward', 'backward']
        p1, p2, p3 = 1 / 3, 1 / 3, 1 / 3

        test_distribution = np.random.choice(test_distribution_set, p=[p1, p2, p3])
        imb_factor = distrb[test_distribution][0]
        reverse = distrb[test_distribution][1]

        ide_train_set = LDDateset(ide_logits, label, imb_ub=100.0, step=step, sampling_method=sampling_method,
                                  imb_factor=imb_factor, reverse=reverse, seed=random.randint(0, 1000))
        ide_train_set.save_param(save_data_path)
    return ide_train_set


def run_one(seed, feat_dir, top_k='ada', num_epoch=100, tol=0.95, force_gen_data=True, step=.1,
            sampling_method='mixed'):
    set_seed(seed)
    save_path = f'{feat_dir}/distri/200epoch_seed{seed}_k_{top_k}_tol{tol}_ind_choice'
    ide_train_set = get_dataset_wrap(feat_dir, top_k, save_path, tol, force_gen_data, step=step,
                                     sampling_method='mixed')

    print(f"ide_train_set.feat.shape: {ide_train_set.feat.shape}")

    ide_loader = DataLoader(ide_train_set, batch_size=1024, shuffle=True, num_workers=0)
    ide_model = Bayesian_LDE(ide_train_set.feat.shape[1], ide_train_set.clc_num, num_samples=10)

    for feat, label in ide_loader:
        print(f"Feature batch shape: {feat.shape}")
        print(f"Label batch shape: {label.shape}")
        break

    optimizer = torch.optim.Adam(ide_model.parameters(), lr=1e-3, betas=(.9, .999))

    ide_model.train()
    for epoch in range(num_epoch):
        tot_loss = 0
        for feat, label in ide_loader:
            output = ide_model(feat)
            #loss = F.kl_div(F.log_softmax(output, dim=1), label, reduction='batchmean')
            mean_output, _ = output
            loss = F.kl_div(F.log_softmax(mean_output, dim=1), label, reduction='batchmean')
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            tot_loss += loss
    ide_model.eval()
    train_feat_mean = ide_train_set.feat.mean(0)
    train_distri = ide_train_set.label_one_hot.float().mean(0)
    pred_train_distri, _ = ide_model(train_feat_mean.reshape(1, -1))
    pred_train_distri = pred_train_distri.flatten()
    train_dist = pse_label_distri_dist(F.softmax(pred_train_distri, dim=0), train_distri)
    print(f"train pse_label_distri_dist: {train_dist}")
    return ide_model


def mask_logits_tensor(all_3_feat, final_tensor, testk, class_num=100):
    for i in range(3):
        one_masked_logits = LDDateset.topk_mask(all_3_feat[:, i], k=testk[i])
        final_tensor[:, i * class_num:(i + 1) * class_num] = one_masked_logits
    return final_tensor


def check_imb(head_prob, med_prob, tail_prob):
    if head_prob > tail_prob and med_prob > tail_prob:
        return 1
    if tail_prob > head_prob and tail_prob > med_prob:
        return -1
    return 0


def max_logits_score(all_logits):
    all_max_logits, _ = torch.max(all_logits, dim=1)
    avg_max_logits = all_max_logits.mean()
    return avg_max_logits


def LDE_adapt(net, lde_model, val_dataset, val_loader, org_logits_path, train_topk=100, force_pred_logits=False):
    param, param_names = PDA.collect_params(net)
    lr = 1e-3
    optimizer = torch.optim.SGD(param, lr, momentum=0.9)
    PDA_model = PDA(net, optimizer=optimizer)
    PDA_model = PDA_model.cuda()
    lde_model = lde_model.cuda()
    lde_model.eval()
    # update bsce weight
    net.div_out = True
    overall_logits = [[], [], [], []]
    overall_targets = []
    feat_list = []
    PDA_model.eval()
    if not os.path.exists(org_logits_path) or force_pred_logits:
        with torch.no_grad():
            for _, dl in enumerate(val_loader):
                images, target = dl[0], dl[1]
                images = images.cuda()
                target = target.cuda()
                output = PDA_model(images, update_weight=False, consistency_reg=False)
                feat2 = output
                feat_all = torch.hstack([feat2[j] for j in range(3)])
                feat_list.append(feat_all)
                overall_targets.append(target.cpu().detach())
        all_feat = torch.cat(feat_list)
        os.makedirs(os.path.dirname(org_logits_path), exist_ok=True)
        torch.save(all_feat, org_logits_path)
    else:
        all_feat = torch.load(org_logits_path)

    class_num = 100
    all_3_feat = all_feat.reshape(-1, 3, class_num)
    final_tensor = torch.zeros_like(all_feat)

    final_tensor_list = []
    infer_distri_list = []

    final_testk = train_topk
    testK_list = [i for i in range(10, 110, 10)] + [train_topk]
    class_num = 100

    for i in testK_list:
        ft = mask_logits_tensor(all_3_feat, torch.zeros_like(all_feat), [i] * 3, class_num=class_num)
        final_tensor_list.append(ft.cpu())
        #infer_distri_list.append(F.softmax(lde_model(ft.mean(dim=0)), dim=0))
        lde_mean, _ = lde_model(ft.mean(dim=0))
        infer_distri_list.append(F.softmax(lde_mean, dim=0))

    all_ft = torch.stack(final_tensor_list).reshape(len(final_tensor_list), -1, 3, class_num)
    all_dist = torch.stack(infer_distri_list)
    mean_ft = all_ft.mean(dim=1).mean(1)

    head_cmp = mean_ft[:, :36].sum(dim=-1) / mean_ft.sum(dim=1)
    med_cmp = mean_ft[:, 36:70].sum(dim=-1) / mean_ft.sum(dim=1)
    tail_cmp = mean_ft[:, 70:].sum(dim=-1) / mean_ft.sum(dim=1)
    mean_dist = all_dist.mean(dim=0)
    head_prob = mean_dist[:36].mean()
    med_prob = mean_dist[36:70].mean()
    tail_prob = mean_dist[70:].mean()
    dist_flag = check_imb(head_prob, med_prob, tail_prob)
    print(f"head prob: {head_prob}\nmed prb: {med_prob}\ntail prob : {tail_prob}")


    if dist_flag == 1:
        head_k = 2
        _, idx = torch.sort(head_cmp)
        final_tensor = sum(final_tensor_list[idx[-i]] for i in range(1, head_k + 1)) / head_k
        lde_mean, lde_uncertainty = lde_model(final_tensor.mean(dim=0).cuda())
        infer_distri = F.softmax(lde_mean, dim=0)

    elif dist_flag == 0:
        final_tensor = torch.ones_like(all_feat)
        infer_distri = torch.ones(class_num, device='cuda')
        lde_uncertainty = torch.zeros_like(infer_distri)

    else:
        tail_k = 1
        _, idx = torch.sort(tail_cmp)
        final_tensor = sum(final_tensor_list[idx[-i]] for i in range(1, tail_k + 1)) / tail_k
        lde_mean, lde_uncertainty = lde_model(final_tensor.mean(dim=0).cuda())
        infer_distri = F.softmax(lde_mean, dim=0)
    print(f"dist flag {dist_flag}")
    print(f"test top k {final_testk}")

    adaptive_weight = torch.exp(-lde_uncertainty)
    #PDA_model.bsce_weight = (infer_distri * adaptive_weight).flatten().detach()
    scaling_factor = (1.0 - adaptive_weight.mean()).item()
    PDA_model.bsce_weight = (infer_distri * adaptive_weight + scaling_factor).detach()

    return PDA_model


def check_data_loader_distribution(data_loader, num_classes=100):
    class_counts = torch.zeros(num_classes, dtype=torch.int32)

    for _, targets in data_loader:
        class_counts += torch.bincount(targets, minlength=num_classes)

    print(f"Class distribution: {class_counts.tolist()}")


def get_cifar100_args():
    parser = argparse.ArgumentParser(description='LSA exps')
    parser.add_argument('--dataset', default='CIFAR-100', help='Name of dataset')
    parser.add_argument('--force_train_one', default=False, type=bool, help='Force train a new classifier')
    parser.add_argument('--force_gen_data', default=False, type=bool, help='Force generate a new dataset')
    parser.add_argument('--draw_pic', default=False, type=bool, help='plot result')
    parser.add_argument('--epoch', default=100, type=int, help='epochs training LDE')
    parser.add_argument('--step', default=0.1, type=float, help='step of the LDDataset')
    parser.add_argument('--topk', default=-1, type=int, help='train topk, -1 denotes adaptive threshold')
    parser.add_argument('--tol', default=0.8, type=float, help='tolerance of the threshold ')
    parser.add_argument('--test_topk', default=25, type=int, help='test topk')
    parser.add_argument('--feat_dir',
                        default='',
                        help='dir of logtis and label')
    parser.add_argument('--model_dir',
                        default='.../pre_train_model/cifar100_lt/best_model.pth',
                        help='dir of pre-trained model')
    parser.add_argument('--data_dir', default='.../data/cifar-100-python',
                        help='path of dataset')
    parser.add_argument('--used_distri', default=['forward50', 'forward25', 'forward10', 'forward5', 'uniform', 'backward5', 'backward10', 'backward25', 'backward50'],
                        nargs='+', help='distribution of testset, list')
    # ['forward50', 'forward25', 'forward10', 'forward5', 'uniform', 'backward5', 'backward10', 'backward25', 'backward50'],
    parser.add_argument('--sampling_method', default='mixed', choices=['single', 'mixed'],
                        help='Method for sampling Dirichlet distributions')
    return parser.parse_args()


def main():
    args = get_cifar100_args()
    used_distri = args.used_distri
    seedlist = []
    batch_size, num_workers = 128, 0
    name_dataset = args.dataset
    num_epoch = args.epoch
    force_train_one = args.force_train_one
    force_gen_data = args.force_gen_data
    sampling_method = args.sampling_method
    if args.topk < 0:
        topk = 'ada'
    else:
        topk = args.topk
    tol = args.tol
    step = args.step
    feat_dir = args.feat_dir
    model_dir = args.model_dir
    data_dir = str(args.data_dir)

    # create imb Validation Dataset
    net = model_load.load_ncl_cifar100_wo_con(model_dir)
    net = net.cuda()
    result = []
    record_str = ""
    log_file = "experiment_results_cifar.txt"

    with open(log_file, 'a') as f:
        for distri in used_distri:
            imb_factor, reverse = distrb_dict[distri]
            top1_meter = AverageMeter('Acc@1', ':6.2f')
            top5_meter = AverageMeter('Acc@5', ':6.2f')
            head_meter = AverageMeter('Head', ':6.2f')
            med_meter = AverageMeter('med', ':6.2f')
            tail_meter = AverageMeter('tail', ':6.2f')

            for s in seedlist:
                set_seed(s)
                model_dir_seed = os.path.join(feat_dir, f'model/lde_{topk}_{tol}_seed{s}')
                if osp.exists(model_dir_seed) and not force_train_one:
                    LDE_model = torch.load(model_dir_seed)
                    print(f"[Main] Loaded pre-trained LDE model from {model_dir}")
                else:
                    print("[Main] Training new LDE model with Dirichlet Mixture Sampling...")
                    LDE_model = run_one(s, feat_dir=feat_dir, top_k=topk, num_epoch=num_epoch, tol=tol,
                                        force_gen_data=force_gen_data, step=step, sampling_method=sampling_method)
                    os.makedirs(osp.dirname(model_dir_seed), exist_ok=True)
                    torch.save(LDE_model, model_dir_seed)

                val_set = cifar100_c_testset(data_dir, te_transforms, imb_factor=imb_factor, reverse=reverse)
                val_set.set_imbalance_data()
                print(f"[Testing Dataset] Class distribution: {val_set.num_per_cls_dict}")

                val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)

                org_logits_path = feat_dir + f'/org_test_logits_{distri}_seed{s}'
                PDA_model = LDE_adapt(net, LDE_model, val_set, val_loader, org_logits_path, train_topk=4)
                use_logits = None
                val_set.num_per_cls_list = [val_set.num_per_cls_dict[i] for i in range(100)]

                net.div_out = False

                top1, top5, *freq = validate(val_loader, PDA_model, None, name_dataset, gpu=0, print_freq=10,
                                             debug=False, mode='eval', use_logits=use_logits)
                top1_meter.update(top1)
                top5_meter.update(top5)
                head_meter.update(freq[0])
                med_meter.update(freq[1])
                tail_meter.update(freq[2])

                print(f"Seed {s} - now status")
                print(f"top1 {top1} | top5 {top5}")
                print("-" * 20)
                print(f"Seed {s} - avg status")
                print(f"top1 {top1_meter.avg} | top5 {top5_meter.avg}")

                f.write(f"Seed {s} - Distribution: {distri} | top1: {top1} | top5: {top5} | ")
                f.write("-" * 40 + "\n")
                # if distri == 'uniform':
                # break

            result_str = f"Status: distribution: {distri} | epoch {num_epoch} | feat_dir {feat_dir}\n" \
                         f"top1 {top1_meter.avg} | top5 {top5_meter.avg}"
            record_str += f"{top1_meter.avg:.2f}\t"
            result.append(result_str)

        print('_' * 40)
        for r in result:
            print(r, end='\n\n')
            print(record_str)


if __name__ == '__main__':
    main()
